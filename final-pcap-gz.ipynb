{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9922344,"sourceType":"datasetVersion","datasetId":6098268}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T14:33:02.459647Z","iopub.execute_input":"2024-11-15T14:33:02.460548Z","iopub.status.idle":"2024-11-15T14:33:02.928229Z","shell.execute_reply.started":"2024-11-15T14:33:02.460486Z","shell.execute_reply":"2024-11-15T14:33:02.927026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# get the data in zipped format\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport os\n\nurl = 'https://data.caida.org/datasets/passive-2019/equinix-nyc/20190117-130000.UTC/'\nusername = 'pes1202201377@pesu.pes.edu'\npassword = 'rasgullabros'\n\nresponse = requests.get(url, auth=(username, password))\nsoup = BeautifulSoup(response.content, 'html.parser')\n\nlinks = soup.find_all('a')\n\nos.makedirs('equinix_nyc_data', exist_ok=True)\n\ndownload_count = 0\n\nfor link in links:\n    file_name = link.get('href')\n    if file_name.endswith('.pcap.gz'):\n        file_url = url + file_name\n        print(f'Downloading {file_name}')\n        file_response = requests.get(file_url, auth=(username, password))\n        \n        with open(os.path.join('equinix_nyc_data', file_name), 'wb') as f:\n            f.write(file_response.content)\n        \n        download_count += 1\n        if download_count == 6:\n            break\n\nprint(\"Download complete\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T04:32:37.934932Z","iopub.execute_input":"2024-11-16T04:32:37.935808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install pyshark nest_asyncio scapy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T09:10:50.448468Z","iopub.execute_input":"2024-11-16T09:10:50.449546Z","iopub.status.idle":"2024-11-16T09:11:05.356664Z","shell.execute_reply.started":"2024-11-16T09:10:50.449490Z","shell.execute_reply":"2024-11-16T09:11:05.355408Z"}},"outputs":[{"name":"stdout","text":"Collecting pyshark\n  Downloading pyshark-0.6-py3-none-any.whl.metadata (806 bytes)\nRequirement already satisfied: nest_asyncio in /opt/conda/lib/python3.10/site-packages (1.6.0)\nCollecting scapy\n  Downloading scapy-2.6.1-py3-none-any.whl.metadata (5.6 kB)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from pyshark) (5.3.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from pyshark) (2.4.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pyshark) (21.3)\nRequirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from pyshark) (1.4.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->pyshark) (3.1.2)\nDownloading pyshark-0.6-py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scapy-2.6.1-py3-none-any.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scapy, pyshark\nSuccessfully installed pyshark-0.6 scapy-2.6.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install gzip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T09:11:05.359398Z","iopub.execute_input":"2024-11-16T09:11:05.360307Z","iopub.status.idle":"2024-11-16T09:11:07.553918Z","shell.execute_reply.started":"2024-11-16T09:11:05.360265Z","shell.execute_reply":"2024-11-16T09:11:07.552671Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement gzip (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for gzip\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# only unzip\n\nimport os\nimport gzip\n\ninput_dir = './equinix_nyc_data'\noutput_dir = './unzipped_data'\n\nos.makedirs(output_dir, exist_ok=True)\n\ndef unzip_gz_files(input_dir, output_dir):\n    for file_name in os.listdir(input_dir):\n        if file_name.endswith('.pcap.gz'):\n            gz_file_path = os.path.join(input_dir, file_name)\n            output_file_path = os.path.join(output_dir, file_name[:-3])\n            try:\n                with gzip.open(gz_file_path, 'rb') as gz_file:\n                    with open(output_file_path, 'wb') as output_file:\n                        output_file.write(gz_file.read())\n                print(f\"Unzipped: {gz_file_path} -> {output_file_path}\")\n            except Exception as e:\n                print(f\"Error unzipping {gz_file_path}: {e}\")\n\nunzip_gz_files(input_dir, output_dir)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install scapy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T10:54:44.082689Z","iopub.execute_input":"2024-11-16T10:54:44.083327Z","iopub.status.idle":"2024-11-16T10:55:01.362343Z","shell.execute_reply.started":"2024-11-16T10:54:44.083248Z","shell.execute_reply":"2024-11-16T10:55:01.360977Z"}},"outputs":[{"name":"stdout","text":"Collecting scapy\n  Downloading scapy-2.6.1-py3-none-any.whl.metadata (5.6 kB)\nDownloading scapy-2.6.1-py3-none-any.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scapy\nSuccessfully installed scapy-2.6.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport scapy.all as scapy\nimport csv\n\ninput_dir = '/kaggle/input/unzipped-pcap-2019/unzipped_data'\nchunk_dir = './chunked_data'\ncsv_dir = './csv_data'\n\nos.makedirs(chunk_dir, exist_ok=True)\nos.makedirs(csv_dir, exist_ok=True)\n\ndef chunk_pcap(input_pcap, chunk_size_mb, output_dir):\n    packets = scapy.rdpcap(input_pcap)\n    chunk_data = []\n    chunk_size = 0\n    chunk_count = 1\n    output_file = os.path.join(output_dir, f\"chunk_{chunk_count}.pcap\")\n\n    for packet in packets:\n        packet_size = len(bytes(packet))\n        chunk_data.append(packet)\n        chunk_size += packet_size / (1024 * 1024)\n        \n        if chunk_size >= chunk_size_mb:\n            scapy.wrpcap(output_file, chunk_data)\n            print(f\"Saved chunk {chunk_count} as {output_file}\")\n            chunk_count += 1\n            chunk_data = []\n            chunk_size = 0\n            output_file = os.path.join(output_dir, f\"chunk_{chunk_count}.pcap\")\n    \n    if chunk_data:\n        scapy.wrpcap(output_file, chunk_data)\n        print(f\"Saved final chunk {chunk_count} as {output_file}\")\n\ndef process_pcap_to_csv(chunk_path, csv_path):\n    try:\n        packets = scapy.rdpcap(chunk_path)\n        with open(csv_path, mode=\"w\", newline=\"\") as file:\n            writer = csv.writer(file)\n            writer.writerow([\"Timestamp\", \"Source\", \"Destination\", \"Protocol\", \"Length\"])\n            for packet in packets:\n                try:\n                    if packet.haslayer(scapy.IP):\n                        writer.writerow([\n                            packet.time,\n                            packet[scapy.IP].src,\n                            packet[scapy.IP].dst,\n                            packet.proto,\n                            len(packet),\n                        ])\n                except AttributeError:\n                    continue\n        print(f\"Processed {chunk_path} -> {csv_path}\")\n    except Exception as e:\n        print(f\"Error processing {chunk_path}: {e}\")\n\npcap_files = [file for file in os.listdir(input_dir) if file.endswith('.pcap')]\nif pcap_files:\n    first_pcap_file = pcap_files[0]\n    input_pcap_path = os.path.join(input_dir, first_pcap_file)\n\n    # Step 1: Chunk the pcap file\n    chunk_pcap(input_pcap_path, chunk_size_mb=100, output_dir=chunk_dir)\n\n    # Step 2: Process each chunk into a CSV\n    for chunk_file in os.listdir(chunk_dir):\n        if chunk_file.startswith(first_pcap_file) and chunk_file.endswith('.pcap'):\n            chunk_path = os.path.join(chunk_dir, chunk_file)\n            csv_file_path = os.path.join(csv_dir, f\"{chunk_file}.csv\")\n            process_pcap_to_csv(chunk_path, csv_file_path)\n\nprint(\"Processing completed for the first .pcap file.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T10:55:01.365074Z","iopub.execute_input":"2024-11-16T10:55:01.365480Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}